{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-20T08:15:59.891508Z","iopub.execute_input":"2022-02-20T08:15:59.892373Z","iopub.status.idle":"2022-02-20T08:15:59.909127Z","shell.execute_reply.started":"2022-02-20T08:15:59.892334Z","shell.execute_reply":"2022-02-20T08:15:59.908045Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\n## For plotting \nimport matplotlib.pyplot as plt\n## For stationarity test and decomposition\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm","metadata":{"execution":{"iopub.status.busy":"2022-02-20T08:16:04.413058Z","iopub.execute_input":"2022-02-20T08:16:04.413393Z","iopub.status.idle":"2022-02-20T08:16:05.303189Z","shell.execute_reply.started":"2022-02-20T08:16:04.413358Z","shell.execute_reply":"2022-02-20T08:16:05.302104Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"thead = pd.read_csv(\"../input/ubiquant-market-prediction/train.csv\", nrows=5) # just read in a few lines to get the column headers\ndtypes = dict(zip(thead.columns.values, ['object', 'int16', 'int16', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32']))\ndel thead\nprint('Datatype used for each column:\\n', json.dumps(dtypes))\ndf = pd.read_csv(\"../input/ubiquant-market-prediction/train.csv\", dtype=dtypes, nrows=100000)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T08:16:09.060960Z","iopub.execute_input":"2022-02-20T08:16:09.061292Z","iopub.status.idle":"2022-02-20T08:16:17.717797Z","shell.execute_reply.started":"2022-02-20T08:16:09.061257Z","shell.execute_reply":"2022-02-20T08:16:17.716625Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#type individual time stamps\ncolumn_values = df[[\"time_id\"]].values\nunique_values =  np.unique(column_values)\nprint(unique_values)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T08:16:37.571971Z","iopub.execute_input":"2022-02-20T08:16:37.572291Z","iopub.status.idle":"2022-02-20T08:16:37.581847Z","shell.execute_reply.started":"2022-02-20T08:16:37.572256Z","shell.execute_reply":"2022-02-20T08:16:37.580797Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def add_shift_time_col(df):\n    #Sort by time_id \n    #print(df)\n    #print(type(df))\n    time_sorted_df = df\n    time_sorted_df = time_sorted_df.sort_values(by ='time_id')\n    #Shift col by one val \n    time_sorted_df['target_shift'] = time_sorted_df['target']\n    #print('Vor shift', time_sorted_df['target_shift'])\n    time_sorted_df['target_shift'] = time_sorted_df.target.shift(1)\n    #print('Nach shift', time_sorted_df['target_shift'])\n    #Aufpassen vllt macht der stress wegen nan wert in erster zelle\n    mean_target_by_asset = time_sorted_df['target_shift'].mean()\n    time_sorted_df['target_shift'].values[0] = mean_target_by_asset\n    #print('Shift hat wert gefüllt', time_sorted_df['target_shift'])\n    return time_sorted_df","metadata":{"execution":{"iopub.status.busy":"2022-02-20T08:16:40.477051Z","iopub.execute_input":"2022-02-20T08:16:40.477404Z","iopub.status.idle":"2022-02-20T08:16:40.484494Z","shell.execute_reply.started":"2022-02-20T08:16:40.477368Z","shell.execute_reply":"2022-02-20T08:16:40.483484Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#In jeder asset klasse korrelation der zeitversetzten returns berechnen, und für die asset klassen mit sehr geringer correlation \n#nehmen wir einen random walk und für die anderen deterministische Modelle\n\n#1 Für jede Asset klasse eine zeitlich geordnete df machen\n#1.1 shift funktion bauen \n#2 dann mit für jedes asset df time shift machen auf den returns\n#3 korrelation berechnen für die returns aller asset klassen\n#4.1 asset klassen mit keiner korrelation auf den returns -> random walk\n#4.2 asset klassen mit einer 'hohen' korrelation lasso laufen lassen, dann random forest oder lin\n\n#split by asset class\n\n#erstmal asset class analytics -> siehe oben\n#print(len(df[[\"investment_id\"]]))\ninvest_id_col_vals = df[[\"investment_id\"]].values\nunique_invest_ids =  np.unique(invest_id_col_vals)\n#print(unique_invest_ids)\n#wir erschaffen ein df dictionary nach allen asset klassen 1...3772\ndict_by_asset_class_shift = {elem : pd.DataFrame for elem in unique_invest_ids}\nfor key in dict_by_asset_class_shift.keys():\n    dict_by_asset_class_shift[key] = df[:][df.investment_id == key]\n    #print('Sollte nach invest_id geordnet', dict_by_asset_class_shift[key])\n    dict_by_asset_class_shift[key] = add_shift_time_col(dict_by_asset_class_shift[key])\n    #print('Sollte nach time geordnet', dict_by_asset_class_shift[key])","metadata":{"execution":{"iopub.status.busy":"2022-02-20T08:17:26.774861Z","iopub.execute_input":"2022-02-20T08:17:26.775182Z","iopub.status.idle":"2022-02-20T08:17:32.731562Z","shell.execute_reply.started":"2022-02-20T08:17:26.775148Z","shell.execute_reply":"2022-02-20T08:17:32.730587Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"low_cor_key = []\nhigh_cor_key = []\n\nfor key in dict_by_asset_class_shift:\n    corrrelation_returns = np.corrcoef(dict_by_asset_class_shift[key]['target_shift'], dict_by_asset_class_shift[key]['target'])[0][1]\n    dict_by_asset_class_shift[key].drop(['target_shift'], axis = 1)\n    if corrrelation_returns < 0.35:\n        low_cor_key.append(key)\n    else:\n        high_cor_key.append(key)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T08:17:39.743561Z","iopub.execute_input":"2022-02-20T08:17:39.743846Z","iopub.status.idle":"2022-02-20T08:17:43.052194Z","shell.execute_reply.started":"2022-02-20T08:17:39.743815Z","shell.execute_reply":"2022-02-20T08:17:43.051043Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#Random walk\nprint(len(low_cor_key))\nprint(len(high_cor_key))\nfor key in low_cor_key:\n    dict_by_asset_class_shift[key] \n    ts = dict_by_asset_class_shift[key].groupby(\"time_id\")[\"target\"].sum().rename(\"returns_t\")\n    #print(ts)\n    ts_train, ts_test = split_train_test(ts)\n    dict_by_asset_class_shift[key]['preds'] = dict_by_asset_class_shift[key]['target']\n    preds = simulate_rw(ts_train, ts_test)\n    print('Run')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T08:41:39.920068Z","iopub.execute_input":"2022-02-20T08:41:39.920478Z","iopub.status.idle":"2022-02-20T08:41:42.964899Z","shell.execute_reply.started":"2022-02-20T08:41:39.920441Z","shell.execute_reply":"2022-02-20T08:41:42.963510Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"#Highly correlated\n#load data\nfrom numpy import mean\nfrom numpy import std\nfrom numpy import absolute\nfrom pandas import read_csv\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import Lasso\n# define model\nmodel = Lasso(alpha=1.0)\n# evaluate an lasso regression model on the dataset\nfor key in high_cor_key:\n    model = Lasso(alpha=1.0)\n    dataframe = dict_by_asset_class_shift[key] \n    data = dataframe.values\n    X = dict_by_asset_class_shift[key].iloc[:,4:303]\n    y = dict_by_asset_class_shift[key]['target']\n    # define model\n    model = Lasso(alpha=1.0)\n    # define model evaluation method\n    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n    # evaluate model\n    scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n    # force scores to be positive\n    scores = absolute(scores)\n    print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))","metadata":{"execution":{"iopub.status.busy":"2022-02-20T09:04:09.483044Z","iopub.execute_input":"2022-02-20T09:04:09.483513Z","iopub.status.idle":"2022-02-20T09:04:09.529425Z","shell.execute_reply.started":"2022-02-20T09:04:09.483476Z","shell.execute_reply":"2022-02-20T09:04:09.528322Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"'''\nGenerate a Random Walk process.\n:parameter\n    :param y0: num - starting value\n    :param n: num - length of process\n    :param ymin: num - limit\n    :param ymax: num - limit\n'''\ndef utils_generate_rw(y0, n, sigma, ymin=None, ymax=None):\n    rw = [y0]\n    for t in range(1, n):\n        yt = rw[t-1] + np.random.normal(0,sigma)\n        if (ymax is not None) and (yt > ymax):\n            yt = rw[t-1] - abs(np.random.normal(0,sigma))\n        elif (ymin is not None) and (yt < ymin):\n            yt = rw[t-1] + abs(np.random.normal(0,sigma))\n        rw.append(yt)\n    return rw","metadata":{"execution":{"iopub.status.busy":"2022-02-20T08:18:28.264166Z","iopub.execute_input":"2022-02-20T08:18:28.264518Z","iopub.status.idle":"2022-02-20T08:18:28.272863Z","shell.execute_reply.started":"2022-02-20T08:18:28.264450Z","shell.execute_reply":"2022-02-20T08:18:28.271740Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"'''\nSplit train/test from any given data point.\n:parameter\n    :param ts: pandas Series\n    :param test: num or str - test size (ex. 0.20) or index position\n                 (ex. \"yyyy-mm-dd\", 1000)\n:return\n    ts_train, ts_test\n'''\ndef split_train_test(ts, test=0.20, plot=True, figsize=(15,5)):\n    ## define splitting point\n    if type(test) is float:\n        split = int(len(ts)*(1-test))\n        perc = test\n    elif type(test) is str:\n        split = ts.reset_index()[ \n                      ts.reset_index().iloc[:,0]==test].index[0]\n        perc = round(len(ts[split:])/len(ts), 2)\n    else:\n        split = test\n        perc = round(len(ts[split:])/len(ts), 2)\n    print(\"--- splitting at index: \", split, \"|\", \n          ts.index[split], \"| test size:\", perc, \" ---\")\n    \n    ## split ts\n    ts_train = ts.head(split)\n    ts_test = ts.tail(len(ts)-split)\n    if plot is True:\n        fig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, \n                               sharey=True, figsize=figsize)\n        ts_train.plot(ax=ax[0], grid=True, title=\"Train\", \n                      color=\"black\")\n        ts_test.plot(ax=ax[1], grid=True, title=\"Test\", \n                     color=\"black\")\n        ax[0].set(xlabel=None)\n        ax[1].set(xlabel=None)\n        plt.show()\n        \n    return ts_train, ts_test","metadata":{"execution":{"iopub.status.busy":"2022-02-20T08:16:55.479040Z","iopub.execute_input":"2022-02-20T08:16:55.479671Z","iopub.status.idle":"2022-02-20T08:16:55.491487Z","shell.execute_reply.started":"2022-02-20T08:16:55.479633Z","shell.execute_reply":"2022-02-20T08:16:55.490661Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def simulate_rw(ts_train, ts_test, figsize=(15,10)):\n    ## simulate train\n    diff_ts = ts_train - ts_train.shift(1)\n    rw = utils_generate_rw(y0=ts_train[0], n=len(ts_train),\n         sigma=diff_ts.std(), ymin=ts_train.min(), \n         ymax=ts_train.max())\n    dtf_train = ts_train.to_frame(name=\"ts\").merge(\n                 pd.DataFrame(rw, index=ts_train.index, \n                              columns=[\"model\"]), \n                              how='left', left_index=True,\n                              right_index=True)\n    ## test\n    rw = utils_generate_rw(y0=ts_train[0], n=len(ts_test), \n                           sigma=diff_ts.std(), ymin=ts_train.min(), \n                           ymax=ts_train.max())\n    dtf_test = ts_test.to_frame(name=\"ts\").merge(\n                 pd.DataFrame(rw, index=ts_test.index, \n                              columns=[\"forecast\"]), how='left', \n                              left_index=True, right_index=True)\n    ## evaluate\n    dtf = dtf_train.append(dtf_test)\n    dtf = utils_evaluate_forecast(dtf, figsize=figsize, \n                                  title=\"Random Walk Simulation\")\n    return dtf","metadata":{"execution":{"iopub.status.busy":"2022-02-20T08:22:01.413044Z","iopub.execute_input":"2022-02-20T08:22:01.414039Z","iopub.status.idle":"2022-02-20T08:22:01.424020Z","shell.execute_reply.started":"2022-02-20T08:22:01.414000Z","shell.execute_reply":"2022-02-20T08:22:01.423121Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"'''\nEvaluation metrics for predictions.\n:parameter\n    :param dtf: DataFrame with columns raw values, fitted training  \n                 values, predicted test values\n:return\n    dataframe with raw ts and forecast\n'''\ndef utils_evaluate_forecast(dtf, title, plot=True, figsize=(20,13)):\n    try:\n        ## residuals\n        dtf[\"residuals\"] = dtf[\"ts\"] - dtf[\"model\"]\n        dtf[\"error\"] = dtf[\"ts\"] - dtf[\"forecast\"]\n        dtf[\"error_pct\"] = dtf[\"error\"] / dtf[\"ts\"]\n        \n        ## kpi\n        residuals_mean = dtf[\"residuals\"].mean()\n        residuals_std = dtf[\"residuals\"].std()\n        error_mean = dtf[\"error\"].mean()\n        error_std = dtf[\"error\"].std()\n        mae = dtf[\"error\"].apply(lambda x: np.abs(x)).mean()\n        mape = dtf[\"error_pct\"].apply(lambda x: np.abs(x)).mean()  \n        mse = dtf[\"error\"].apply(lambda x: x**2).mean()\n        rmse = np.sqrt(mse)  #root mean squared error\n        \n        ## intervals\n        dtf[\"conf_int_low\"] = dtf[\"forecast\"] - 1.96*residuals_std\n        dtf[\"conf_int_up\"] = dtf[\"forecast\"] + 1.96*residuals_std\n        dtf[\"pred_int_low\"] = dtf[\"forecast\"] - 1.96*error_std\n        dtf[\"pred_int_up\"] = dtf[\"forecast\"] + 1.96*error_std\n        \n        ## plot\n        if plot==True:\n            fig = plt.figure(figsize=figsize)\n            fig.suptitle(title, fontsize=20)   \n            ax1 = fig.add_subplot(2,2, 1)\n            ax2 = fig.add_subplot(2,2, 2, sharey=ax1)\n            ax3 = fig.add_subplot(2,2, 3)\n            ax4 = fig.add_subplot(2,2, 4)\n            ### training\n            dtf[pd.notnull(dtf[\"model\"])][[\"ts\",\"model\"]].plot(color=[\"black\",\"green\"], title=\"Model\", grid=True, ax=ax1)      \n            ax1.set(xlabel=None)\n            ### test\n            dtf[pd.isnull(dtf[\"model\"])][[\"ts\",\"forecast\"]].plot(color=[\"black\",\"red\"], title=\"Forecast\", grid=True, ax=ax2)\n            ax2.fill_between(x=dtf.index, y1=dtf['pred_int_low'], y2=dtf['pred_int_up'], color='b', alpha=0.2)\n            ax2.fill_between(x=dtf.index, y1=dtf['conf_int_low'], y2=dtf['conf_int_up'], color='b', alpha=0.3)     \n            ax2.set(xlabel=None)\n            ### residuals\n            dtf[[\"residuals\",\"error\"]].plot(ax=ax3, color=[\"green\",\"red\"], title=\"Residuals\", grid=True)\n            ax3.set(xlabel=None)\n            ### residuals distribution\n            dtf[[\"residuals\",\"error\"]].plot(ax=ax4, color=[\"green\",\"red\"], kind='kde', title=\"Residuals Distribution\", grid=True)\n            ax4.set(ylabel=None)\n            plt.show()\n            print(\"Training --> Residuals mean:\", np.round(residuals_mean), \" | std:\", np.round(residuals_std))\n            print(\"Test --> Error mean:\", np.round(error_mean), \" | std:\", np.round(error_std),\n                  \" | mae:\",np.round(mae), \" | mape:\",np.round(mape*100), \"%  | mse:\",np.round(mse), \" | rmse:\",np.round(rmse))\n        \n        return dtf[[\"ts\",\"model\",\"residuals\",\"conf_int_low\",\"conf_int_up\", \n                    \"forecast\",\"error\",\"pred_int_low\",\"pred_int_up\"]]\n    \n    except Exception as e:\n        print(\"--- got error ---\")\n        print(e)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T08:22:46.713563Z","iopub.execute_input":"2022-02-20T08:22:46.714076Z","iopub.status.idle":"2022-02-20T08:22:46.736580Z","shell.execute_reply.started":"2022-02-20T08:22:46.714033Z","shell.execute_reply":"2022-02-20T08:22:46.735643Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\nfor (test_df, sample_prediction_df) in iter_test:\n    dtest = test_df.iloc[:,2:] #ADDED: remove row_id and investment_id\n    pred = model.predict(dtest) #ADDED\n    sample_prediction_df['target'] = pred  # CHANGED: replace 0 with pred\n    env.predict(sample_prediction_df)   # register your predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sample_prediction_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}