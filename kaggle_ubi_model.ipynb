{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-25T02:29:23.435362Z","iopub.execute_input":"2022-02-25T02:29:23.436105Z","iopub.status.idle":"2022-02-25T02:29:23.471439Z","shell.execute_reply.started":"2022-02-25T02:29:23.435962Z","shell.execute_reply":"2022-02-25T02:29:23.470394Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\n## For plotting \nimport matplotlib.pyplot as plt\n## For stationarity test and decomposition\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm","metadata":{"execution":{"iopub.status.busy":"2022-02-25T02:31:15.717399Z","iopub.execute_input":"2022-02-25T02:31:15.717720Z","iopub.status.idle":"2022-02-25T02:31:17.481275Z","shell.execute_reply.started":"2022-02-25T02:31:15.717689Z","shell.execute_reply":"2022-02-25T02:31:17.480219Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"thead = pd.read_csv(\"../input/ubiquant-market-prediction/train.csv\", nrows=5) # just read in a few lines to get the column headers\ndtypes = dict(zip(thead.columns.values, ['object', 'int16', 'int16', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32', 'float32']))\ndel thead\nprint('Datatype used for each column:\\n', json.dumps(dtypes))\ndf = pd.read_csv(\"../input/ubiquant-market-prediction/train.csv\", dtype=dtypes, nrows=100000)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T02:31:20.489333Z","iopub.execute_input":"2022-02-25T02:31:20.489671Z","iopub.status.idle":"2022-02-25T02:31:33.553237Z","shell.execute_reply.started":"2022-02-25T02:31:20.489634Z","shell.execute_reply":"2022-02-25T02:31:33.552162Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#type individual time stamps\ncolumn_values = df[[\"time_id\"]].values\nunique_values =  np.unique(column_values)\nprint(unique_values)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T02:31:44.579790Z","iopub.execute_input":"2022-02-25T02:31:44.580953Z","iopub.status.idle":"2022-02-25T02:31:44.596192Z","shell.execute_reply.started":"2022-02-25T02:31:44.580895Z","shell.execute_reply":"2022-02-25T02:31:44.595071Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def add_shift_time_col(df):\n    #Sort by time_id \n    #print(df)\n    #print(type(df))\n    time_sorted_df = df\n    time_sorted_df = time_sorted_df.sort_values(by ='time_id')\n    #Shift col by one val \n    time_sorted_df['target_shift'] = time_sorted_df['target']\n    #print('Vor shift', time_sorted_df['target_shift'])\n    time_sorted_df['target_shift'] = time_sorted_df.target.shift(1)\n    #print('Nach shift', time_sorted_df['target_shift'])\n    #Aufpassen vllt macht der stress wegen nan wert in erster zelle\n    mean_target_by_asset = time_sorted_df['target_shift'].mean()\n    time_sorted_df['target_shift'].values[0] = mean_target_by_asset\n    #print('Shift hat wert gefüllt', time_sorted_df['target_shift'])\n    return time_sorted_df","metadata":{"execution":{"iopub.status.busy":"2022-02-25T02:31:56.078207Z","iopub.execute_input":"2022-02-25T02:31:56.079116Z","iopub.status.idle":"2022-02-25T02:31:56.085608Z","shell.execute_reply.started":"2022-02-25T02:31:56.079069Z","shell.execute_reply":"2022-02-25T02:31:56.084513Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#In jeder asset klasse korrelation der zeitversetzten returns berechnen, und für die asset klassen mit sehr geringer correlation \n#nehmen wir einen random walk und für die anderen deterministische Modelle\n\n#1 Für jede Asset klasse eine zeitlich geordnete df machen\n#1.1 shift funktion bauen \n#2 dann mit für jedes asset df time shift machen auf den returns\n#3 korrelation berechnen für die returns aller asset klassen\n#4.1 asset klassen mit keiner korrelation auf den returns -> random walk\n#4.2 asset klassen mit einer 'hohen' korrelation lasso laufen lassen, dann random forest oder lin\n\n#split by asset class\n\n#erstmal asset class analytics -> siehe oben\n#print(len(df[[\"investment_id\"]]))\ninvest_id_col_vals = df[[\"investment_id\"]].values\nunique_invest_ids =  np.unique(invest_id_col_vals)\n#print(unique_invest_ids)\n#wir erschaffen ein df dictionary nach allen asset klassen 1...3772\ndict_by_asset_class_shift = {elem : pd.DataFrame for elem in unique_invest_ids}\nfor key in dict_by_asset_class_shift.keys():\n    dict_by_asset_class_shift[key] = df[:][df.investment_id == key]\n    #print('Sollte nach invest_id geordnet', dict_by_asset_class_shift[key])\n    dict_by_asset_class_shift[key] = add_shift_time_col(dict_by_asset_class_shift[key])\n    #print('Sollte nach time geordnet', dict_by_asset_class_shift[key])","metadata":{"execution":{"iopub.status.busy":"2022-02-25T02:32:00.912391Z","iopub.execute_input":"2022-02-25T02:32:00.912679Z","iopub.status.idle":"2022-02-25T02:32:05.628043Z","shell.execute_reply.started":"2022-02-25T02:32:00.912648Z","shell.execute_reply":"2022-02-25T02:32:05.627227Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"low_cor_key = []\nhigh_cor_key = []\n\nfor key in dict_by_asset_class_shift:\n    corrrelation_returns = np.corrcoef(dict_by_asset_class_shift[key]['target_shift'], dict_by_asset_class_shift[key]['target'])[0][1]\n    dict_by_asset_class_shift[key].drop(['target_shift'], axis = 1)\n    if corrrelation_returns < 0.35:\n        low_cor_key.append(key)\n    else:\n        high_cor_key.append(key)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T02:32:11.366482Z","iopub.execute_input":"2022-02-25T02:32:11.366818Z","iopub.status.idle":"2022-02-25T02:32:14.038434Z","shell.execute_reply.started":"2022-02-25T02:32:11.366782Z","shell.execute_reply":"2022-02-25T02:32:14.037464Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n#print(len(low_cor_key))\nhigh_cor_count_list = []\nfor key in high_cor_key:\n    #print(len(dict_by_asset_class_shift[key]))\n    high_cor_count_list.append(len(dict_by_asset_class_shift[key]))\nhigh_cor_count_list_array = np.array(high_cor_count_list)\ncounter_high_cor = Counter(high_cor_count_list_array)\n\n#plot histogram for number of observations in asset class (x-axis) and the frequency particular observation numbers occur (y-axis)\nimport matplotlib.pyplot as plt\nplt.bar(counter_high_cor.keys(), counter_high_cor.values())","metadata":{"execution":{"iopub.status.busy":"2022-02-25T02:50:28.467444Z","iopub.execute_input":"2022-02-25T02:50:28.467861Z","iopub.status.idle":"2022-02-25T02:50:29.027713Z","shell.execute_reply.started":"2022-02-25T02:50:28.467820Z","shell.execute_reply":"2022-02-25T02:50:29.026775Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#Wie gehe ich damit um, dass es investment_ids gibt, die nur eine row enthalten und ids die mehrere rows enthalten\n#ich muss die rausnehmen, die kaum werte haben und hoch korrelieren\n#wir schauen uns an wie die frequency dist \nfrom collections import Counter\n#print(len(low_cor_key))\nlow_cor_count_list = []\nfor key in low_cor_key:\n    #print(len(dict_by_asset_class_shift[key]))\n    low_cor_count_list.append(len(dict_by_asset_class_shift[key]))\nlow_cor_count_list_array = np.array(low_cor_count_list)\ncounter_low_cor = Counter(low_cor_count_list_array)\n\n#plot histogram for number of observations in asset class (x-axis) and the frequency particular observation numbers occur (y-axis)\nimport matplotlib.pyplot as plt\nplt.bar(counter_low_cor.keys(), counter_low_cor.values())","metadata":{"execution":{"iopub.status.busy":"2022-02-25T02:47:42.177775Z","iopub.execute_input":"2022-02-25T02:47:42.178157Z","iopub.status.idle":"2022-02-25T02:47:42.549793Z","shell.execute_reply.started":"2022-02-25T02:47:42.178117Z","shell.execute_reply":"2022-02-25T02:47:42.548587Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"dict_by_asset_class_shift[10]['pred_target']","metadata":{"execution":{"iopub.status.busy":"2022-02-20T10:27:19.041366Z","iopub.execute_input":"2022-02-20T10:27:19.041664Z","iopub.status.idle":"2022-02-20T10:27:19.050951Z","shell.execute_reply.started":"2022-02-20T10:27:19.041633Z","shell.execute_reply":"2022-02-20T10:27:19.0498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Highly correlated\n#load data\nfrom numpy import mean\nfrom numpy import std\nfrom numpy import absolute\nfrom pandas import read_csv\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import Lasso\n# define model\nprint(len(high_cor_key))\nfor key in high_cor_key:\n    print(len(dict_by_asset_class_shift[key]))\nprint(len(high_cor_key))\nmodel = Lasso(alpha=1.0)\n# evaluate an lasso regression model on the dataset\nfor key in high_cor_key:\n    print(dict_by_asset_class_shift[key] )\n    if(len(dict_by_asset_class_shift[key]) < 10):\n        continue\n    model = Lasso(alpha=1.0)\n    dataframe = dict_by_asset_class_shift[key] \n    data = dataframe.values\n    X = dict_by_asset_class_shift[key].loc[:,\"f_0\":\"f_299\"]\n    y = dict_by_asset_class_shift[key]['target']\n    # define model\n    model = Lasso(alpha=1.0)\n    # define model evaluation method\n    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n    # evaluate model\n    scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n    # force scores to be positive\n    scores = absolute(scores)\n    print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\nfor (test_df, sample_prediction_df) in iter_test:\n    dtest = test_df.iloc[:,2:] #ADDED: remove row_id and investment_id\n    pred = model.predict(dtest) #ADDED\n    sample_prediction_df['target'] = pred  # CHANGED: replace 0 with pred\n    env.predict(sample_prediction_df)   # register your predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sample_prediction_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}